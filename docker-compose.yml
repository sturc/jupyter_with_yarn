version: "3"
services:
  hadoop:
    build: ./hadoop
    image: "hadoop:python3"
    container_name: "hadoop"
    networks:
      hadoop_cluster:
        ipv4_address: "192.168.11.2"
    ports:
      - "8020:8020"
      - "8042:8042"
      - "8088:8088" 
      - "9000:9000"
      - "19888:19888" 
      - "50070:50070" 
      - "50075:50075"  
    # stdin_open: true
    # tty: true
  jupyter:
    build: 
      context: https://github.com/jupyter/docker-stacks.git#0f2a7473c41f2c865e836d70f8ff8c9c7b7ed28c:pyspark-notebook
      args:
      - spark_version=3.0.1
      - hadoop_version=2.7
      - spark_checksum=F4A10BAEC5B8FF1841F10651CAC2C4AA39C162D3029CA180A9749149E6060805B5B5DDF9287B4AA321434810172F8CC0534943AC005531BB48B6622FBE228DDC 
      - openjdk_version=8 
      - py4j_version=0.10.9
    image: "pyspark-notebook:spark-3.0.1"
    container_name: "notebook"
    environment:
      - HADOOP_CONF_DIR=/home/jovyan/hadoop-configs/
    ports:
      - "8888:8888"
    volumes:
       - "/Users/sturm/Development/python/ds_bd_ss20/ds_bd:/home/jovyan/work"  
       - "/Users/sturm/Development/python/ds_bd_ss20/ds_bd/hadoop-configs:/home/jovyan/hadoop-configs"
       - "/Users/sturm/Development/jupyter_with_yarn/pyspark-notebook/before-notebook:/usr/local/bin/before-notebook.d/"
    networks:
      hadoop_cluster:
        ipv4_address: "192.168.11.3"   
networks:
  hadoop_cluster:
    driver: bridge
    ipam:
      driver: default
      config:
      - subnet:  192.168.11.0/24